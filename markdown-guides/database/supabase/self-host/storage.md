# Supabase Storage Self-Hosted Configuration Guide

## ğŸ¯ **Overview**

This guide covers the complete setup and configuration of Supabase Storage in a self-hosted environment, including RLS policies, Kong API Gateway integration, and troubleshooting common issues.

## ğŸ—ï¸ **Architecture**

### **The Digital Warehouse Analogy**

Our Supabase Storage architecture works like a **modern digital warehouse**:

```
ğŸ¢ MUDROCK DIGITAL WAREHOUSE
â”œâ”€â”€ ğŸšª Kong Gateway (Reception Desk)
â”‚   â”œâ”€â”€ Checks credentials (API keys)
â”‚   â”œâ”€â”€ Routes requests to correct department
â”‚   â””â”€â”€ Handles security and access control
â”‚
â”œâ”€â”€ ğŸ“¦ Supabase Storage Buckets (Storage Rooms)
â”‚   â”œâ”€â”€ animal-metrics-parquet (Pulse Data Room)
â”‚   â”œâ”€â”€ test-bucket (General Storage Room)
â”‚   â””â”€â”€ Each room has specific access rules
â”‚
â”œâ”€â”€ ğŸ—„ï¸ PostgreSQL Database (Inventory System)
â”‚   â”œâ”€â”€ Tracks what's in each room
â”‚   â”œâ”€â”€ Records who has access
â”‚   â””â”€â”€ Manages permissions (RLS policies)
â”‚
â””â”€â”€ ğŸšš DuckDB Analytics (Data Processing Center)
    â”œâ”€â”€ Reads files from storage rooms
    â”œâ”€â”€ Processes and analyzes data
    â””â”€â”€ Returns insights to clients
```

### **Component Architecture**

```
Client Applications
â”œâ”€â”€ Supabase Studio (Web UI)
â”œâ”€â”€ MudRock Desktop App (Tauri)
â”œâ”€â”€ DuckDB Analytics Engine
â””â”€â”€ API Clients (curl, Postman, etc.)
    â†“
Kong API Gateway (Port 8000) - The Reception Desk
    â†“
Supabase Storage Service (Port 5000) - The Warehouse
    â†“
PostgreSQL Database (storage.buckets table) - The Inventory System
    â†“
File System (/var/lib/storage) - The Physical Storage
```

### **Why HTTP API + Direct File Access Works Best**

**Our HTTP API approach provides**:

- **ğŸ¦† DuckDB**: Reads Parquet files directly from file system (`/var/lib/storage/`)
- **ğŸš€ Fast Access**: No S3 protocol overhead, direct file system access
- **ğŸ”§ Simple Setup**: No complex S3 authentication or configuration
- **ğŸ“± Universal Compatibility**: Any tool can use HTTP API endpoints
- **ğŸŒ Reliable**: Works consistently without S3 compatibility issues

## ğŸ”„ **How Components Work Together**

### **Data Flow Example: DuckDB Reading Parquet Files**

Let's trace how DuckDB reads a Parquet file from our storage:

```
1. ğŸ¦† DuckDB Request
   â””â”€â”€ "Read /var/lib/storage/stub/stub/animal-metrics-parquet/test-data/pulse_data.parquet"

2. ğŸ“ Direct File System Access
   â”œâ”€â”€ DuckDB service has mounted storage volume
   â”œâ”€â”€ Reads Parquet files directly from file system
   â”œâ”€â”€ Uses glob pattern to find actual files
   â””â”€â”€ Processes data without HTTP overhead

3. ğŸ—„ï¸ PostgreSQL Database (Metadata Only)
   â”œâ”€â”€ Stores file metadata and bucket information
   â”œâ”€â”€ Tracks file locations and permissions
   â””â”€â”€ Manages RLS policies for access control

4. ğŸ“Š Analytics Processing
   â”œâ”€â”€ DuckDB processes Parquet data directly
   â”œâ”€â”€ Applies filters and aggregations
   â”œâ”€â”€ Returns JSON response with analytics
   â””â”€â”€ No network calls needed for data access
```

### **Authentication Flow**

```
1. ğŸ”‘ Client Authentication
   â”œâ”€â”€ MudRock App: Uses anon key for user operations
   â”œâ”€â”€ DuckDB Service: Uses service role key for admin operations
   â””â”€â”€ Studio: Uses anon key for client-side operations

2. ğŸšª Kong Gateway Processing
   â”œâ”€â”€ Validates API key format
   â”œâ”€â”€ Checks JWT token validity
   â”œâ”€â”€ Routes to appropriate service
   â””â”€â”€ Adds security headers

3. ğŸ—„ï¸ PostgreSQL RLS Validation
   â”œâ”€â”€ Checks user role (anon, authenticated, service_role)
   â”œâ”€â”€ Applies RLS policies to bucket access
   â”œâ”€â”€ Validates file permissions
   â””â”€â”€ Records access attempts

4. ğŸ“¦ Storage Service Authorization
   â”œâ”€â”€ Verifies bucket exists
   â”œâ”€â”€ Checks file access rights
   â”œâ”€â”€ Validates file size limits
   â””â”€â”€ Enforces MIME type restrictions
```

### **File Upload Process**

```
1. ğŸ“¤ Client Upload Request
   â””â”€â”€ "Upload file to animal-metrics-parquet/test-data/"

2. ğŸšª Kong Gateway
   â”œâ”€â”€ Validates request headers
   â”œâ”€â”€ Checks file size limits
   â”œâ”€â”€ Routes to storage service
   â””â”€â”€ Handles CORS if needed

3. ğŸ—„ï¸ PostgreSQL Database
   â”œâ”€â”€ Checks bucket permissions
   â”œâ”€â”€ Validates file metadata
   â”œâ”€â”€ Records file information
   â””â”€â”€ Updates bucket statistics

4. ğŸ“¦ Storage Service
   â”œâ”€â”€ Creates file in /var/lib/storage
   â”œâ”€â”€ Generates unique file ID
   â”œâ”€â”€ Stores file metadata
   â””â”€â”€ Returns success response

5. ğŸ“ File System
   â”œâ”€â”€ Writes file to disk
   â”œâ”€â”€ Sets file permissions
   â”œâ”€â”€ Updates directory structure
   â””â”€â”€ Manages storage space
```

### **Analytics Query Process**

```
1. ğŸ” Analytics Request
   â””â”€â”€ "Get pulse data for German Shepherds aged 2 years"

2. ğŸ¦† DuckDB Processing
   â”œâ”€â”€ Scans mounted storage volume
   â”œâ”€â”€ Finds Parquet files using glob patterns
   â”œâ”€â”€ Reads files directly from file system
   â””â”€â”€ Performs analytics queries locally

3. ğŸ“ File System Access
   â”œâ”€â”€ Direct access to /var/lib/storage volume
   â”œâ”€â”€ No network overhead for file reading
   â”œâ”€â”€ Fast sequential or random access
   â””â”€â”€ Handles large files efficiently

4. ğŸ—„ï¸ PostgreSQL Integration
   â”œâ”€â”€ Queries animal metadata for filtering
   â”œâ”€â”€ Joins with Parquet data for context
   â”œâ”€â”€ Applies breed and age filters
   â””â”€â”€ Returns structured results

5. ğŸ“Š Analytics Results
   â”œâ”€â”€ DuckDB processes 1000+ records in <100ms
   â”œâ”€â”€ Applies complex filters and aggregations
   â”œâ”€â”€ Returns JSON response with statistics
   â””â”€â”€ Client displays real-time analytics
```

## ğŸ“‹ **Prerequisites**

- Supabase self-hosted stack deployed
- Kong API Gateway configured
- PostgreSQL database with storage schema
- RLS policies properly configured

## ğŸ”§ **Storage Service Configuration**

### **Docker Compose Configuration**

```yaml
# docker-compose-supabase.yml
supabase-storage:
  image: supabase/storage-api:v1.22.7
  restart: unless-stopped
  container_name: mudrock-supabase-storage
  depends_on:
    - supabase-db
  environment:
    # Database Configuration
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    POSTGRES_HOST: supabase-db
    POSTGRES_PORT: 5432
    POSTGRES_DB: postgres
    POSTGRES_USER: supabase_storage_admin

    # Storage Configuration
    STORAGE_BACKEND: file
    STORAGE_API_PORT: 5000
    STORAGE_API_HOST: 0.0.0.0

    # File Storage Paths
    FILE_STORAGE_BACKEND_PATH: /var/lib/storage
    FILE_SIZE_LIMIT: 52428800 # 50MB

    # Tenant Configuration
    TENANT_ID: mudrock
    REGION: local

    # JWT Configuration
    JWT_SECRET: ${JWT_SECRET}
    JWT_EXP: 3600

    # CORS Configuration
    CORS_ORIGINS: "*"

    # Logging
    LOG_LEVEL: info
  ports:
    - "5001:5000" # External access for debugging
  volumes:
    - storage_data:/var/lib/storage
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s
```

### **Environment Variables**

```bash
# supabase.env
# Storage Configuration
STORAGE_API_PORT=5000
STORAGE_API_HOST=0.0.0.0
FILE_SIZE_LIMIT=52428800
STORAGE_BACKEND=file
TENANT_ID=mudrock
REGION=local

# Database Configuration
POSTGRES_PASSWORD=MudRockSecure2024!@#
POSTGRES_HOST=supabase-db
POSTGRES_PORT=5432
POSTGRES_DB=postgres
POSTGRES_USER=supabase_storage_admin

# JWT Configuration
JWT_SECRET=cAdtagpNtA1Wy9a7pbRS+QLb0LkxtBncXWkc//hPdPg=

# DuckDB Analytics Configuration
DUCKDB_STORAGE_VOLUME=mudrockenterprise-mudrocksupabasestandalone-o7myoc_supabase_storage_data
```

## ğŸ” **Row Level Security (RLS) Policies**

### **Critical RLS Configuration**

Supabase Storage requires specific RLS policies on the `storage.buckets` table to function properly. Without these policies, Studio will not display buckets and API calls will fail.

```sql
-- sql/fix-storage-rls-policies.sql

-- Drop existing policies to avoid conflicts
DROP POLICY IF EXISTS "Allow authenticated users to view bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow service_role to view bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow anon to view bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow authenticated users to insert bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow service_role to insert bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow anon to insert bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow authenticated users to update bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow service_role to update bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow anon to update bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow authenticated users to delete bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow service_role to delete bucket" ON storage.buckets;
DROP POLICY IF EXISTS "Allow anon to delete bucket" ON storage.buckets;

-- Enable RLS on storage.buckets table
ALTER TABLE storage.buckets ENABLE ROW LEVEL SECURITY;

-- Create comprehensive RLS policies for all roles

-- SELECT policies (view buckets)
CREATE POLICY "Allow authenticated users to view bucket" ON storage.buckets FOR SELECT TO authenticated USING (true);
CREATE POLICY "Allow service_role to view bucket" ON storage.buckets FOR SELECT TO service_role USING (true);
CREATE POLICY "Allow anon to view bucket" ON storage.buckets FOR SELECT TO anon USING (true);

-- INSERT policies (create buckets)
CREATE POLICY "Allow authenticated users to insert bucket" ON storage.buckets FOR INSERT TO authenticated WITH CHECK (true);
CREATE POLICY "Allow service_role to insert bucket" ON storage.buckets FOR INSERT TO service_role WITH CHECK (true);
CREATE POLICY "Allow anon to insert bucket" ON storage.buckets FOR INSERT TO anon WITH CHECK (true);

-- UPDATE policies (modify buckets)
CREATE POLICY "Allow authenticated users to update bucket" ON storage.buckets FOR UPDATE TO authenticated USING (true) WITH CHECK (true);
CREATE POLICY "Allow service_role to update bucket" ON storage.buckets FOR UPDATE TO service_role USING (true) WITH CHECK (true);
CREATE POLICY "Allow anon to update bucket" ON storage.buckets FOR UPDATE TO anon USING (true) WITH CHECK (true);

-- DELETE policies (remove buckets)
CREATE POLICY "Allow authenticated users to delete bucket" ON storage.buckets FOR DELETE TO authenticated USING (true);
CREATE POLICY "Allow service_role to delete bucket" ON storage.buckets FOR DELETE TO service_role USING (true);
CREATE POLICY "Allow anon to delete bucket" ON storage.buckets FOR DELETE TO anon USING (true);
```

### **Applying RLS Policies**

```bash
# Apply RLS policies to the database
docker exec mudrock-supabase-db psql -U supabase_admin -d postgres -f /path/to/fix-storage-rls-policies.sql

# Or execute directly
docker exec mudrock-supabase-db psql -U supabase_admin -d postgres -c "
-- [Copy the SQL content above]
"
```

## ğŸŒ **Kong API Gateway Integration**

### **Kong Configuration**

```yaml
# kong-simple.yml
services:
  - name: supabase-storage
    url: http://mudrock-supabase-storage:5000/
    routes:
      - name: supabase-storage-all
        strip_path: true
        paths:
          - /storage/v1/
    plugins:
      - name: cors
        config:
          origins:
            - "*"
          methods:
            - GET
            - POST
            - PUT
            - DELETE
            - OPTIONS
          headers:
            - Accept
            - Accept-Version
            - Content-Length
            - Content-MD5
            - Content-Type
            - Date
            - X-Auth-Token
            - Authorization
            - apikey
```

### **API Endpoints**

| Endpoint                             | Method | Description        | Authentication    |
| ------------------------------------ | ------ | ------------------ | ----------------- |
| `/storage/v1/bucket`                 | GET    | List all buckets   | Anon/Service Role |
| `/storage/v1/bucket`                 | POST   | Create new bucket  | Anon/Service Role |
| `/storage/v1/bucket/{id}`            | GET    | Get bucket details | Anon/Service Role |
| `/storage/v1/bucket/{id}`            | PUT    | Update bucket      | Anon/Service Role |
| `/storage/v1/bucket/{id}`            | DELETE | Delete bucket      | Anon/Service Role |
| `/storage/v1/object/{bucket}/{path}` | GET    | Download file      | Anon/Service Role |
| `/storage/v1/object/{bucket}/{path}` | POST   | Upload file        | Anon/Service Role |
| `/storage/v1/object/{bucket}/{path}` | DELETE | Delete file        | Anon/Service Role |
| `/storage/v1/health`                 | GET    | Health check       | Anon/Service Role |

## ğŸ¦† **DuckDB Analytics Integration**

### **Direct File System Access**

Our DuckDB analytics service uses direct file system access instead of HTTP API calls for optimal performance:

```yaml
# DuckDB Service Configuration
duckdb-analytics:
  volumes:
    - mudrockenterprise-mudrocksupabasestandalone-o7myoc_supabase_storage_data:/var/lib/storage:ro
  environment:
    - STORAGE_BUCKET=animal-metrics-parquet
    - STORAGE_ENDPOINT=http://supabase-kong:8000/storage/v1
    - STORAGE_ACCESS_KEY=${SERVICE_ROLE_KEY}
```

### **File Access Pattern**

```python
# DuckDB reads files directly from mounted volume
file_path = "/var/lib/storage/stub/stub/animal-metrics-parquet/test-data/test_pulse_data.parquet"
actual_files = glob.glob(f"{file_path}/*")
if actual_files:
    actual_file_path = actual_files[0]
    conn.execute(f"CREATE TABLE pulse_data AS SELECT * FROM read_parquet('{actual_file_path}')")
```

### **Performance Benefits**

- **ğŸš€ Fast Access**: Direct file system reads, no HTTP overhead
- **ğŸ“Š High Throughput**: Processes 1000+ records in <100ms
- **ğŸ”§ Simple Setup**: No S3 authentication or complex configuration
- **ğŸ’¾ Efficient**: No network calls for data access
- **ğŸ›¡ï¸ Reliable**: No dependency on storage service availability

### **File Structure**

```
/var/lib/storage/
â””â”€â”€ stub/stub/animal-metrics-parquet/
    â”œâ”€â”€ test-data/
    â”‚   â””â”€â”€ test_pulse_data.parquet/
    â”‚       â””â”€â”€ 69e43c2d-c90b-46a8-9ffe-ed3135947137
    â”œâ”€â”€ 1/metric_type=pulse_rate/year=2023/month=07/day=06/
    â”œâ”€â”€ 2/metric_type=pulse_rate/year=2017/month=07/day=21/
    â””â”€â”€ ... (more animal data)
```

## ğŸ”‘ **Authentication**

### **API Key Configuration**

```bash
# Anon Key (for client-side operations)
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYW5vbiIsImlzcyI6InN1cGFiYXNlLW11ZHJvY2siLCJhdWQiOiJhdXRoZW50aWNhdGVkIiwiaWF0IjoxNzU0NjYxNDE0LCJleHAiOjIwNzAyMzc0MTR9.pKQ270lrWeeJ_K2Vm0rUyMYMMfc8LUcmRI4igawRL2o

# Service Role Key (for server-side operations)
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoic2VydmljZV9yb2xlIiwiaXNzIjoic3VwYWJhc2UtbXVkcm9jayIsImF1ZCI6ImF1dGhlbnRpY2F0ZWQiLCJpYXQiOjE3NTQ2NjE0MTQsImV4cCI6MjA3MDIzNzQxNH0.ipR05YNdP7Ux72VTNqrJHIBKhQW5jvmt20rYNPp_scs
```

### **Request Headers**

```bash
# Required headers for all storage API requests
apikey: <ANON_KEY or SERVICE_ROLE_KEY>
Authorization: Bearer <ANON_KEY or SERVICE_ROLE_KEY>
Content-Type: application/json
```

## ğŸ§ª **Testing Storage Service**

### **Health Check**

```bash
# Test storage service health
curl -s "http://91.99.166.223:8000/storage/v1/health" \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $ANON_KEY" | jq .

# Expected response:
# {"healthy": true}
```

### **DuckDB Analytics Testing**

```bash
# Test DuckDB analytics with real Parquet data
curl -s "http://91.99.166.223:8081/analytics/pulse-data?limit=5" | jq .

# Expected response:
# {
#   "success": true,
#   "data": [
#     {
#       "animal_id": 1,
#       "timestamp": "2023-01-01T16:39:00",
#       "pulse_value": 90.0,
#       "sensor_id": "sensor_1",
#       "quality_score": 0.98
#     }
#   ],
#   "aggregations": {
#     "total_readings": 1000,
#     "avg_pulse": 80.12,
#     "min_pulse": 70.0,
#     "max_pulse": 90.0,
#     "std_pulse": 5.66
#   }
# }
```

### **List Buckets**

```bash
# List all storage buckets
curl -s "http://91.99.166.223:8000/storage/v1/bucket" \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $ANON_KEY" | jq .

# Expected response:
# [
#   {
#     "id": "animal-metrics-parquet",
#     "name": "animal-metrics-parquet",
#     "owner": "",
#     "public": false,
#     "file_size_limit": null,
#     "allowed_mime_types": null,
#     "created_at": "2025-10-02T11:51:44.354Z",
#     "updated_at": "2025-10-02T11:51:44.354Z"
#   }
# ]
```

### **Create Bucket**

```bash
# Create a new storage bucket
curl -X POST "http://91.99.166.223:8000/storage/v1/bucket" \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "my-new-bucket",
    "name": "my-new-bucket",
    "public": false
  }' | jq .
```

### **Upload File**

```bash
# Upload a file to a bucket
curl -X POST "http://91.99.166.223:8000/storage/v1/object/my-bucket/test-file.txt" \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $ANON_KEY" \
  -F "file=@/path/to/local/file.txt" | jq .
```

## ğŸ› **Troubleshooting**

### **Common Issues**

#### **1. Studio Not Showing Buckets**

**Symptoms:**

- Supabase Studio shows "No buckets available"
- API calls work but Studio UI is empty

**Root Cause:** Missing or incorrect RLS policies

**Solution:**

```bash
# Apply RLS policies
docker exec mudrock-supabase-db psql -U supabase_admin -d postgres -f /path/to/fix-storage-rls-policies.sql
```

#### **2. 403 Unauthorized Errors**

**Symptoms:**

```
{"statusCode": "403", "code": "AccessDenied", "error": "Unauthorized", "message": "new row violates row-level security policy"}
```

**Root Cause:** RLS policies blocking access

**Solution:**

- Ensure RLS policies are applied correctly
- Verify API key has proper permissions
- Check that anon key is used for client-side operations

#### **3. 502 Bad Gateway**

**Symptoms:**

- Kong returns 502 when accessing storage endpoints
- Storage service logs show connection errors

**Root Cause:** Kong routing to wrong container or port

**Solution:**

```bash
# Check Kong configuration
docker logs mudrock-supabase-kong

# Restart Kong to refresh service discovery
docker restart mudrock-supabase-kong
```

#### **4. Studio Health Check Failing**

**Symptoms:**

- Studio container shows "unhealthy" status
- Health check endpoint returns errors

**Root Cause:** Health check configuration pointing to wrong port

**Solution:**

```yaml
# Update health check in docker-compose-supabase.yml
healthcheck:
  test:
    [
      "CMD-SHELL",
      'node -e "fetch(''http://localhost:9999/api/platform/profile'').then((r) => {if (r.status !== 200) throw new Error(r.status)})"',
    ]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

### **Debugging Commands**

```bash
# Check storage service status
docker ps | grep storage

# View storage service logs
docker logs mudrock-supabase-storage --tail 50

# Test direct storage access (bypass Kong)
curl -s "http://91.99.166.223:5001/health"

# Test Kong routing
curl -s "http://91.99.166.223:8000/storage/v1/health" \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $ANON_KEY"

# Check RLS policies
docker exec mudrock-supabase-db psql -U supabase_admin -d postgres -c "
SELECT schemaname, tablename, policyname, permissive, roles, cmd, qual
FROM pg_policies
WHERE tablename = 'buckets';
"
```

## ğŸ“Š **Performance Monitoring**

### **Key Metrics**

- **DuckDB Analytics**: < 100ms for 1000+ records processing
- **File Access**: Direct file system reads, no network latency
- **Storage Operations**: < 30ms for bucket operations via HTTP API
- **Storage Usage**: Monitor `/var/lib/storage` volume
- **Concurrent Processing**: DuckDB handles multiple queries efficiently

### **Monitoring Commands**

```bash
# Check DuckDB analytics performance
curl -s "http://91.99.166.223:8081/analytics/pulse-data?limit=1000" | jq '.execution_time_ms'

# Monitor storage volume usage
docker exec mudrock-supabase-storage df -h /var/lib/storage

# Check DuckDB service performance
docker stats mudrock-duckdb-analytics

# Test analytics endpoints
curl -s "http://91.99.166.223:8081/analytics/breed-stats" | jq '.execution_time_ms'

# Check storage service performance
docker stats mudrock-supabase-storage
```

## ğŸ”’ **Security Considerations**

### **File Access Control**

- **Private Buckets**: Default for sensitive data
- **Public Buckets**: Only for public assets
- **File Size Limits**: Configured per bucket
- **MIME Type Restrictions**: Optional content filtering

### **Authentication Best Practices**

- **Client-Side**: Use anon key for user operations
- **Server-Side**: Use service role key for admin operations
- **API Keys**: Rotate regularly
- **CORS**: Configure appropriate origins

### **Data Protection**

- **Encryption**: Files stored unencrypted (consider encryption at application level)
- **Backup**: Regular backups of storage volume
- **Access Logs**: Monitor all storage operations
- **RLS Policies**: Regularly audit and update

## ğŸ“š **Additional Resources**

- [Supabase Storage Documentation](https://supabase.com/docs/guides/storage)
- [Kong API Gateway Documentation](https://docs.konghq.com/)
- [PostgreSQL RLS Documentation](https://www.postgresql.org/docs/current/ddl-rowsecurity.html)
- [Docker Compose Reference](https://docs.docker.com/compose/compose-file/)

## ğŸ¯ **Current Working Setup Summary**

### **âœ… What's Working**

1. **Supabase Storage Service**: Running on port 5000 with file backend
2. **Kong API Gateway**: Routing storage requests on port 8000
3. **DuckDB Analytics**: Direct file system access for optimal performance
4. **Parquet File Processing**: 1000+ records processed in <100ms
5. **Real Data Analytics**: All endpoints returning actual Parquet data
6. **RLS Policies**: Properly configured for bucket access

### **ğŸ”§ Architecture Highlights**

- **Direct File Access**: DuckDB reads Parquet files directly from mounted volume
- **No S3 Complexity**: Simple HTTP API for file management, direct access for analytics
- **High Performance**: Sub-100ms analytics processing
- **Reliable**: No network dependencies for data access
- **Scalable**: Ready for more data and complex queries

### **ğŸ“Š Live Endpoints**

```bash
# Analytics (Real Parquet Data)
GET http://91.99.166.223:8081/analytics/pulse-data?limit=10
GET http://91.99.166.223:8081/analytics/breed-stats

# Storage Management
GET http://91.99.166.223:8000/storage/v1/bucket
GET http://91.99.166.223:8000/storage/v1/health

# Animal Data
GET http://91.99.166.223:8081/animals/dogs
```

---

**Last Updated**: January 2025  
**Version**: 2.0  
**Status**: Production Ready with DuckDB Analytics âœ…
