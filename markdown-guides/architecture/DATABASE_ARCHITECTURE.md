# MudRock Database Architecture

## ğŸ¯ **Overview**

MudRock uses a **hybrid PostgreSQL + Parquet architecture** that combines the best of both worlds: fast metadata queries and efficient large data storage.

## ğŸ—ï¸ **Current Architecture**

### **Hybrid PostgreSQL + Parquet Approach**

#### **1. PostgreSQL for Metadata & Relationships**
```sql
-- Fast queries on structured data
SELECT w.name, b.name as basin, COUNT(l.id) as log_count
FROM wells w 
JOIN basins b ON w.basin_id = b.id
LEFT JOIN logs l ON w.id = l.well_id
GROUP BY w.id, b.name;
```

#### **2. Parquet Files for Large Data**
- **Already implemented** in your `parquet_files` table!
- Perfect for 10,000+ depth-value pairs
- Columnar storage = fast analytical queries
- Compression (Snappy/GZIP) = 70-90% size reduction
- Time series data ready

#### **3. Local Storage Benefits**
- **Data sovereignty** - your data stays local
- **No auth complexity** - single user
- **Direct file system access** - faster than cloud
- **Offline capability** - work anywhere

## ğŸ“Š **Database Schema Analysis**

### **Core Tables Found:**
- `basins`, `wells`, `logs`, `log_types` - Core subsurface data
- `parquet_files` - **Already using Parquet!** ğŸ‰
- `geological_models`, `surfaces`, `surface_points` - 3D modeling
- `zones`, `zone_cutoffs` - Geological zoning
- `fluid_sets`, `markers` - Petrophysical data
- `teams`, `projects`, `project_members` - Collaboration
- `preferences`, `profiles` - User settings
- `data_access_log` - Audit trail

## ğŸš€ **Implementation Strategy**

### **Option A: Local-First (Recommended for MVP)**
```typescript
// Local PostgreSQL + Local Parquet Files
const config = {
  database: 'postgresql://localhost:5432/mudrock',
  parquetStorage: './data/parquet/',
  compression: 'snappy'
}
```

**Benefits:**
- âœ… No auth needed
- âœ… Full data control
- âœ… Faster file access
- âœ… Works offline
- âœ… Simple deployment

### **Option B: Supabase Hybrid**
```typescript
// Supabase PostgreSQL + Supabase Storage
const config = {
  supabaseUrl: 'https://ptlyfnkyfxwlzfefhbcc.supabase.co',
  supabaseKey: 'your-anon-key',
  storageBucket: 'parquet-files'
}
```

**Benefits:**
- âœ… Cloud backup
- âœ… Multi-user ready
- âœ… Real-time subscriptions
- âŒ Auth complexity
- âŒ Network dependency

## ğŸ“ˆ **Time Series Data Strategy**

### **For LLM Queries:**
1. **Small time series** â†’ PostgreSQL `time_series_data` table
2. **Large time series** â†’ Parquet files with metadata in PostgreSQL
3. **Analytics** â†’ Pre-computed aggregations in PostgreSQL

### **Example Time Series Structure:**
```sql
-- PostgreSQL metadata
CREATE TABLE time_series_logs (
  id UUID PRIMARY KEY,
  name TEXT,
  parquet_file_path TEXT,
  time_range TSRANGE,
  interval TEXT, -- '1 minute', '1 hour'
  columns JSONB
);

-- Parquet file: /data/parquet/well_001_pressure.parquet
-- Columns: timestamp, pressure, temperature, quality_flag
```

## ğŸ¯ **Recommended Implementation**

### **1. Use Local PostgreSQL + Parquet**
- Install PostgreSQL locally
- Use `schema_hybrid.sql` (already created)
- Store parquet files in `./data/parquet/`

### **2. Data Flow:**
```
LAS/SEG-Y Files â†’ Convert to Parquet â†’ Store locally
                â†“
            PostgreSQL metadata
                â†“
            Tauri app queries
```

### **3. Migration Path:**
1. **Start local** (MVP)
2. **Export Supabase data** when needed
3. **Keep Supabase as backup** option

## ğŸ“Š **Performance Comparison**

| Data Type | PostgreSQL | Parquet | Hybrid |
|-----------|------------|---------|---------|
| Metadata (1KB) | âš¡ Fast | âŒ Slow | âš¡ Fast |
| Log Data (10MB) | ğŸŒ Slow | âš¡ Fast | âš¡ Fast |
| Time Series (100MB) | ğŸŒ Very Slow | âš¡ Fast | âš¡ Fast |
| Analytics | âš¡ Fast | âš¡ Fast | âš¡ Fast |

## ğŸ”§ **Technical Implementation**

### **PostgreSQL Schema**
```sql
-- Core tables for metadata
CREATE TABLE wells (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    team_id UUID,
    x DOUBLE PRECISION,
    y DOUBLE PRECISION,
    project_id UUID,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Parquet file metadata
CREATE TABLE parquet_files (
    id UUID PRIMARY KEY,
    well_id INTEGER REFERENCES wells(id),
    file_path TEXT NOT NULL,
    file_size BIGINT,
    compression TEXT,
    columns JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### **Parquet File Structure**
```
/data/parquet/
â”œâ”€â”€ well_001/
â”‚   â”œâ”€â”€ pressure.parquet
â”‚   â”œâ”€â”€ temperature.parquet
â”‚   â””â”€â”€ flow_rate.parquet
â”œâ”€â”€ well_002/
â”‚   â”œâ”€â”€ pressure.parquet
â”‚   â””â”€â”€ temperature.parquet
â””â”€â”€ metadata.json
```

## ğŸ¯ **Benefits of Hybrid Approach**

### **âœ… Performance Benefits**
- **Fast Metadata Queries**: PostgreSQL for relationships and small data
- **Efficient Large Data**: Parquet for time series and log data
- **Columnar Storage**: Fast analytical queries on large datasets
- **Compression**: 70-90% size reduction

### **âœ… Operational Benefits**
- **Local Control**: Data stays on your machine
- **Offline Capable**: Work without internet
- **No Auth Complexity**: Single user setup
- **Direct Access**: Faster than cloud storage

### **âœ… Development Benefits**
- **Consistent Environment**: Same setup everywhere
- **Easy Testing**: Local data for development
- **Version Control**: Schema changes tracked in Git
- **Backup Simple**: File system backups

## ğŸš€ **Next Steps**

### **Immediate**
1. **Install local PostgreSQL**
2. **Use `schema_hybrid.sql`** (already created)
3. **Create parquet file utilities**
4. **Migrate from Supabase** (optional)

### **Short Term**
1. **Implement parquet file management**
2. **Add data conversion utilities**
3. **Create backup/restore procedures**
4. **Optimize query performance**

### **Long Term**
1. **Add data compression options**
2. **Implement data versioning**
3. **Add data validation**
4. **Create data migration tools**

## ğŸ¯ **Conclusion**

The hybrid PostgreSQL + Parquet approach gives you the best of both worlds:

- âœ… **Fast metadata queries** with PostgreSQL
- âœ… **Efficient large data storage** with Parquet
- âœ… **Local data control** for privacy and performance
- âœ… **Offline capability** for field work
- âœ… **Simple setup** for individual users

**Perfect for geoscience applications!** ğŸ›¢ï¸ 